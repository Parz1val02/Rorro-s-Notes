### Crawlers
---
Crawlers search the web in order to discover domains. Once they find a domain, it will index the entire content of the domain, looking for keywords and other information. The crawler then sends this info to the search engine to be stored for later search

_!Important_
Website/web-server owners themselves stipulate what content crawlers can scrape

__Search engine optimisation__
Search engines will ==prioritise== those domains that easier to index
>Factors that describe an optimal domain:
>	How responsive the website is
>	How easy to crawl the website is through the use of sitemaps
>	Type of keywords in the website

#### Robots.txt
This is the first file indexed by crawlers when visiting a page
This text file defines the permissions the crawler has to the website, like accessing files and directories, or allowing a certain type of crawler with _user-agent_
#### Sitemaps
Sitemaps are resources that assist crawlers, as they specify the necessary routes to find content on the domain. Sitemaps are XML formatted

---